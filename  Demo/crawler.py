import requests
import pandas as pd
from tqdm import tqdm
import time

# C·∫•u h√¨nh API v√† URL
API_URL = "https://ms.vietnamworks.com/job-search/v1.0/search"
BASE_URL = "https://www.vietnamworks.com"


def fetch_page(page: int, hits_per_page: int = 50, max_retries: int = 3):

#   G·ª≠i request ƒë·∫øn API VietnamWorks ƒë·ªÉ l·∫•y d·ªØ li·ªáu job c·ªßa 1 trang.
#   C√≥ retry n·∫øu l·ªói m·∫°ng ho·∫∑c API t·∫°m ng·∫Øt.

    payload = {
        "userId": 0,
        "query": "",
        "filter": [
            {"field": "jobFunction", "value": '[{"parentId":5,"childrenIds":[-1]}]'}  # ng√†nh CNTT
        ],
        "ranges": [],
        "order": [],
        "page": page,
        "hitsPerPage": hits_per_page,
        "retrieveFields": [
            "jobTitle", "companyName", "prettySalary", "skills",
            "alias", "jobId", "approvedOn", "expiredOn"
        ]
    }

    headers = {
        "Content-Type": "application/json",
        "Accept": "application/vnd.api+json",
        # Th√™m d√≤ng n√†y ƒë·ªÉ gi·∫£ l·∫≠p tr√¨nh duy·ªát, tr√°nh l·ªói 403 Forbidden
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    for attempt in range(max_retries):
        try:
            r = requests.post(API_URL, json=payload, headers=headers, timeout=10)
            r.raise_for_status()
            return r.json()
        except requests.RequestException as e:
            print(f"L·ªói khi t·∫£i trang {page}: {e}. Th·ª≠ l·∫°i ({attempt + 1}/{max_retries})...")
            time.sleep(2)
    print(f"‚ùå B·ªè qua trang {page} sau {max_retries} l·∫ßn th·ª≠ th·∫•t b·∫°i.")
    return {"data": []}

# Crawl
def crawl_all():
    print("üöÄ B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu...")
    start = time.time()

    # L·∫•y trang ƒë·∫ßu ƒë·ªÉ bi·∫øt t·ªïng s·ªë trang
    first_page = fetch_page(0)
    meta = first_page.get("meta", {})
    nb_pages = meta.get("nbPages", 1)

    all_jobs = []

    for page in tqdm(range(nb_pages), desc="ƒêang crawl d·ªØ li·ªáu"):
        data = fetch_page(page)
        # Sleep nh·∫π ƒë·ªÉ an to√†n
        time.sleep(0.5)

        for job in data.get("data", []):
            jobTitle = job.get("jobTitle", "").strip()
            companyName = job.get("companyName", "").strip()
            prettySalary = job.get("prettySalary", "Th∆∞∆°ng l∆∞·ª£ng")
            skills = ", ".join([s.get("skillName", "") for s in job.get("skills", [])]) if job.get("skills") else ""

            alias = job.get("alias", "")
            jobId = job.get("jobId", "")
            jobUrl = f"{BASE_URL}/{alias}-{jobId}-jv" if alias and jobId else ""

            approvedOn = job.get("approvedOn", "")
            expiredOn = job.get("expiredOn", "")

            all_jobs.append({
                "jobTitle": jobTitle,
                "companyName": companyName,
                "salary": prettySalary,
                "skills": skills,
                "jobUrl": jobUrl,
                "approvedOn": approvedOn,
                "expiredOn": expiredOn
            })

    df = pd.DataFrame(all_jobs)
    df.to_csv("vnworks_it_jobs.csv", index=False, encoding="utf-8-sig")
    df.to_excel("vnworks_it_jobs.xlsx", index=False)

    print(f"\n‚úÖ ƒê√£ crawl {len(df)} jobs t·ª´ {nb_pages} trang v√†o file vnworks_it_jobs.csv.")
    print(f"‚è± Th·ªùi gian th·ª±c hi·ªán: {time.time() - start:.2f} gi√¢y.")
    print(df.head())
    return df


if __name__ == "__main__":
    crawl_all()